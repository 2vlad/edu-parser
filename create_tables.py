#!/usr/bin/env python3
"""Create Supabase database tables."""

import os
from dotenv import load_dotenv
from supabase import create_client, Client

load_dotenv()

def create_tables():
    """Create the required database tables."""
    
    url = os.environ.get("SUPABASE_URL")
    key = os.environ.get("SUPABASE_KEY")
    
    if not url or not key:
        print("‚ùå Missing SUPABASE_URL or SUPABASE_KEY in .env file")
        return False
    
    print(f"üîó Connecting to Supabase...")
    
    try:
        supabase: Client = create_client(url, key)
        print("‚úÖ Connected to Supabase!")
        
        # Note: The Python client can't execute DDL statements directly
        # We need to use the Supabase SQL Editor for table creation
        
        print("\n‚ö†Ô∏è  Cannot create tables using Python client.")
        print("Please follow these steps:")
        print("\n1. Go to your Supabase project dashboard")
        print("2. Navigate to SQL Editor")
        print("3. Create a new query")
        print("4. Paste and run this SQL:\n")
        
        sql = """
-- Create applicant_counts table
CREATE TABLE applicant_counts (
    id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
    scraper_id TEXT NOT NULL,
    name TEXT NOT NULL,
    count INTEGER,
    status TEXT NOT NULL CHECK (status IN ('success', 'error')),
    error TEXT,
    date DATE NOT NULL DEFAULT CURRENT_DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    synced_to_sheets BOOLEAN DEFAULT FALSE
);

-- Create scrapers_config table
CREATE TABLE scrapers_config (
    scraper_id TEXT PRIMARY KEY,
    name TEXT NOT NULL,
    enabled BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Create indexes for performance
CREATE INDEX idx_applicant_counts_date ON applicant_counts(date);
CREATE INDEX idx_applicant_counts_scraper ON applicant_counts(scraper_id);
CREATE INDEX idx_applicant_counts_sync ON applicant_counts(synced_to_sheets);
"""
        
        print(sql)
        print("\n5. Then run this SQL to add initial scraper configurations:\n")
        
        initial_data_sql = """
-- HSE scrapers
INSERT INTO scrapers_config (scraper_id, name) VALUES
('hse_online_big_data_analytics', '–í–®–≠ - –û–ù–õ–ê–ô–ù –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö'),
('hse_online_data_analytics', '–í–®–≠ - –û–ù–õ–ê–ô–ù –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–∏–∫–ª–∞–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞'),
('hse_online_ai', '–í–®–≠ - –û–ù–õ–ê–ô–ù –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç'),
('hse_online_legaltech', '–í–®–≠ - –û–ù–õ–ê–ô–ù –õ–∏–≥–∞–ª–¢–µ—Ö'),
('hse_online_data_science_master', '–í–®–≠ - –û–ù–õ–ê–ô–ù –ú–∞–≥–∏—Å—Ç—Ä –ø–æ –Ω–∞—É–∫–∞–º –æ –¥–∞–Ω–Ω—ã–º'),
('hse_online_game_engineering', '–í–®–≠ - –û–ù–õ–ê–ô–ù –¶–∏—Ñ—Ä–æ–≤–∞—è –∏–Ω–∂–µ–Ω–µ—Ä–∏—è –¥–ª—è –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∏–≥—Ä'),
('hse_online_urban_analytics', '–í–®–≠ - –û–ù–õ–ê–ô–ù –¶–∏—Ñ—Ä–æ–≤–∞—è —É—Ä–±–∞–Ω–∏—Å—Ç–∏–∫–∞ –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –≥–æ—Ä–æ–¥–∞'),
('hse_online_data_engineering', '–í–®–≠ - –û–ù–õ–ê–ô–ù –ò–Ω–∂–µ–Ω–µ—Ä–∏—è –¥–∞–Ω–Ω—ã—Ö'),
('hse_online_cybersecurity', '–í–®–≠ - –û–ù–õ–ê–ô–ù –ö–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å'),
('hse_online_innovation_business', '–í–®–≠ - –û–ù–õ–ê–ô–ù –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–º –±–∏–∑–Ω–µ—Å–æ–º'),
('hse_online_social_psychology', '–í–®–≠ - –û–ù–õ–ê–ô–ù –ü—Ä–∏–∫–ª–∞–¥–Ω–∞—è —Å–æ—Ü–∏–∞–ª—å–Ω–∞—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—è'),
('hse_online_economic_analysis', '–í–®–≠ - –û–ù–õ–ê–ô–ù –≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑'),
('hse_online_marketing_management', '–í–®–≠ - –û–ù–õ–ê–ô–ù –ú–∞—Ä–∫–µ—Ç–∏–Ω–≥ - –º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç'),
('hse_online_creative_industries', '–í–®–≠ - –û–ù–õ–ê–ô–ù –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤ –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã—Ö –∏–Ω–¥—É—Å—Ç—Ä–∏—è—Ö'),
('hse_online_digital_product', '–í–®–≠ - –û–ù–õ–ê–ô–ù –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ü–∏—Ñ—Ä–æ–≤—ã–º –ø—Ä–æ–¥—É–∫—Ç–æ–º'),
('hse_online_business_analytics', '–í–®–≠ - –û–ù–õ–ê–ô–ù –ú–∞–≥–∏—Å—Ç—Ä –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –±–∏–∑–Ω–µ—Å–∞');

-- MIPT scrapers
INSERT INTO scrapers_config (scraper_id, name) VALUES
('mipt_data_science', '–ú–§–¢–ò - –ù–∞—É–∫–∏ –æ –¥–∞–Ω–Ω—ã—Ö'),
('mipt_modern_combinatorics', '–ú–§–¢–ò - –°–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ç–æ—Ä–∏–∫–∞'),
('mipt_combinatorics_digital_economy', '–ú–§–¢–ò - –ö–æ–º–±–∏–Ω–∞—Ç–æ—Ä–∏–∫–∞ –∏ —Ü–∏—Ñ—Ä–æ–≤–∞—è —ç–∫–æ–Ω–æ–º–∏–∫–∞'),
('mipt_contemporary_combinatorics', '–ú–§–¢–ò - Contemporary combinatorics'),
('mipt_modern_ai', '–ú–§–¢–ò - Modern Artificial Intelligence'),
('mipt_it_product_development', '–ú–§–¢–ò - –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ IT-–ø—Ä–æ–¥—É–∫—Ç–∞'),
('mipt_it_product_management', '–ú–§–¢–ò - –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ IT-–ø—Ä–æ–¥—É–∫—Ç–∞–º–∏');

-- MEPhI scrapers
INSERT INTO scrapers_config (scraper_id, name) VALUES
('mephi_machine_learning', '–ú–ò–§–ò - –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ'),
('mephi_data_science', '–ú–ò–§–ò - –ù–∞—É–∫–∏ –æ –¥–∞–Ω–Ω—ã—Ö'),
('mephi_cybersecurity', '–ú–ò–§–ò - –ö–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å'),
('mephi_info_security', '–ú–ò–§–ò - –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º'),
('mephi_software_development', '–ú–ò–§–ò - –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è'),
('mephi_web_development', '–ú–ò–§–ò - –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –≤–µ–± –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π');
"""
        
        print(initial_data_sql)
        print("\n6. After running both SQL statements, run this script again to test!")
        
        return False  # Tables not created yet
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return False

if __name__ == "__main__":
    create_tables()
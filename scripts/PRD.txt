Вот обновленный PRD с добавленными деталями по парсингу:

# PRD: Система автоматического сбора данных об абитуриентах

## Контекст
Необходимо автоматизировать ежедневный ручной процесс сбора данных о количестве поданных заявлений абитуриентов с ~30 сайтов различных вузов. Сейчас это делается вручную каждый день.

## Цель
Создать надёжную систему, которая:
- Автоматически собирает данные с сайтов вузов раз в сутки
- Сохраняет данные в облачное хранилище
- Позволяет легко добавлять/удалять программы для мониторинга
- Синхронизирует данные с Google Sheets для аналитики

## Функциональные требования

### Сбор данных
- **Парсинг HTML-страниц с таблицами абитуриентов**
  - Поиск таблицы по CSS-селекторам или HTML-структуре
  - Извлечение последней строки таблицы
  - Получение порядкового номера из первой ячейки
  - Гибкий поиск элементов (по классу, id, или структуре)
  
- **Загрузка и парсинг Excel файлов**
  - Прямое чтение Excel по URL без сохранения на диск
  - Подсчет количества строк или извлечение номера последней строки
  - Поддержка разных листов в файле
  
- **Извлечение числа поданных заявлений**
  - Очистка текста от лишних символов
  - Валидация извлеченного числа
  - Обработка разных форматов представления чисел
  
- **Поддержка ~30 различных источников**

### Хранение
- Сохранение: дата, вуз, программа, количество заявлений, статус
- История всех запусков
- Флаг синхронизации с Google Sheets

### Управление
- Включение/отключение отдельных программ без изменения кода
- Добавление новых программ через написание функции-скрапера
- Просмотр статуса через веб-интерфейс

### Синхронизация с Google Sheets
- Ежедневная попытка выгрузки в существующий spreadsheet
- Может падать без влияния на основной процесс
- Повторные попытки при неудаче

## Технический стек
- Язык: Python
- Хостинг: Railway (cron jobs)
- БД: Supabase
- Парсинг: httpx, BeautifulSoup, pandas
- Google Sheets: google-api-python-client

## Архитектура
```
scrapers/
  msu.py      # Скраперы МГУ
  hse.py      # Скраперы ВШЭ
  ...
core/
  storage.py  # Работа с Supabase
  runner.py   # Запуск с изоляцией ошибок
  registry.py # Реестр скраперов
sync/
  sheets_sync.py # Синхронизация с Google Sheets
main.py       # Основной cron job
sheets_sync_job.py # Отдельный cron job для Sheets
```

## Примеры реализации скраперов

### Базовая структура скрапера для HTML
```python
import httpx
from bs4 import BeautifulSoup
import re

def scrape_university_html(url, scraper_id, name):
    try:
        response = httpx.get(url, timeout=30)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Гибкий поиск таблицы
        table = soup.find('table', {'class': lambda x: x and 'applicant' in x.lower()})
        if not table:
            table = soup.find('table')  # Берём первую таблицу
        
        rows = table.find_all('tr')
        last_row = rows[-1]
        
        # Извлекаем номер с валидацией
        first_cell = last_row.find('td')
        text = first_cell.text.strip()
        number_match = re.search(r'\d+', text)
        
        if number_match:
            count = int(number_match.group())
            return {
                'scraper_id': scraper_id,
                'name': name,
                'count': count,
                'status': 'success'
            }
    except Exception as e:
        return {
            'scraper_id': scraper_id,
            'name': name,
            'count': None,
            'status': 'error',
            'error': str(e)
        }
```

### Базовая структура скрапера для Excel
```python
import pandas as pd

def scrape_university_excel(url, scraper_id, name, sheet_name=None):
    try:
        # pandas автоматически скачает файл
        df = pd.read_excel(url, sheet_name=sheet_name)
        
        # Вариант 1: количество строк
        count = len(df)
        
        # Вариант 2: номер из последней строки
        # count = int(df.iloc[-1, 0])
        
        return {
            'scraper_id': scraper_id,
            'name': name,
            'count': count,
            'status': 'success'
        }
    except Exception as e:
        return {
            'scraper_id': scraper_id,
            'name': name,
            'count': None,
            'status': 'error',
            'error': str(e)
        }
```

## База данных (Supabase)

### Таблица applicant_counts
- id (uuid)
- scraper_id (text)
- name (text) - "МГУ - Математика"
- count (integer, nullable)
- status (text) - "success" / "error"
- error (text, nullable)
- date (date)
- created_at (timestamp)
- synced_to_sheets (boolean)

### Таблица scrapers_config
- scraper_id (text, primary key)
- name (text)
- enabled (boolean)
- created_at (timestamp)

## Этапы разработки

### Этап 1: MVP (2-3 дня)
- Настройка Supabase
- Базовая структура проекта
- 3-5 скраперов для тестирования
- core/storage.py и core/runner.py
- Деплой на Railway с cron

### Этап 2: Масштабирование (3-4 дня)
- Добавление остальных 25+ скраперов
- Система мониторинга ошибок
- Простой веб-дашборд для просмотра статуса
- Оптимизация параллельного запуска

### Этап 3: Google Sheets (2 дня)
- Service account для Google API
- sheets_sync.py модуль
- Отдельный cron job для синхронизации
- Обработка ошибок и повторные попытки

## Критерии успеха
- Надёжность: 95%+ успешных запусков скраперов
- Изоляция: падение одного скрапера не влияет на остальные
- Простота добавления: новый вуз = новая функция + строка в конфиге
- Видимость: можно видеть статус без лезть в логи
- Автономность: работает без вмешательства минимум месяц

## Особые требования
- Отказоустойчивость: если скрапер не смог получить данные, это не должно ломать остальные
- Модульность: каждый вуз в отдельном файле для удобства поддержки
- Независимость от Google Sheets: основной функционал работает даже если Sheets недоступен
- Простота: минимум абстракций, прямолинейный код

## Примечания для разработки
- Использовать vibe coding подход с Claude Code
- Начать с простейшей версии, итеративно улучшать
- Приоритет надёжности над элегантностью кода
- Таймауты на все сетевые запросы
- Подробное логирование для отладки

## Примечания для разработки
- Использовать vibe coding подход с Claude Code
- Начать с простейшей версии, итеративно улучшать
- Приоритет надёжности над элегантностью кода
- Таймауты на все сетевые запросы
- Подробное логирование для отладки

### Принципы разработки
- **Строгое следование этапам**: Не переходим к следующему этапу, пока текущий не работает стабильно. Каждый этап должен быть полностью протестирован перед началом следующего.
- **Git для контроля версий**: 
  - Коммит после каждого работающего изменения
  - Понятные сообщения коммитов (например: "Добавлен скрапер МГУ математика")
  - Тег для каждого завершенного этапа (v1-mvp, v2-scaling, v3-sheets)
  - Возможность быстрого отката при проблемах

### Дополнительные принципы кода
- **Явные зависимости**: Все импорты в начале файла
- **Константы вместо магических чисел**: `TIMEOUT = 30` вместо хардкода
- **Единообразие**: Все скраперы следуют одному шаблону
- **Документирование особенностей**: Комментарий если сайт требует особой обработки
- **Тестовый запуск**: Возможность запустить отдельный скрапер для проверки

### Контрольные точки
**Перед переходом к следующему этапу убеждаемся:**
- Все скраперы текущего этапа работают
- Данные корректно сохраняются в БД
- Ошибки обрабатываются и не ломают систему
- Логи понятные и информативные
- Код закоммичен и протегирован

Это обеспечит стабильную, предсказуемую разработку с возможностью отката на любой рабочий этап.

### Что парсим и сохраняем

## Вышка

— ссылка на эксель файл: https://priem45.hse.ru/ABITREPORTS/MAGREPORTS/FullTime/39121437.xls
— смотрим на колонку «Количество заявлений (места с оплатой стоимости обучения)»
— какие программы нужно отслеживать (строки):

ОНЛАЙН Аналитика больших данных
ОНЛАЙН Аналитика данных и прикладная статистика
ОНЛАЙН Искусственный интеллект
ОНЛАЙН ЛигалТех
ОНЛАЙН Магистр по наукам о данных
ОНЛАЙН Цифровая инженерия для компьютерных игр
ОНЛАЙН Цифровая урбанистика и аналитика города
ОНЛАЙН Инженерия данных
ОНЛАЙН Кибербезопасность
ОНЛАЙН Управление инновационным бизнесом
ОНЛАЙН Прикладная социальная психология
ОНЛАЙН Экономический анализ
ОНЛАЙН Маркетинг - менеджмент
ОНЛАЙН Управление в креативных индустриях
ОНЛАЙН Управление цифровым продуктом
ОНЛАЙН Магистр аналитики бизнеса

## МФТИ

— смотрим на последний элемент с классом «R19», в нём значение data-index="_ ЗНАЧЕНИЕ _" +1
— например <tr class="R19" data-index="199"> — забираем 200
— такое значение нужно для ниже перечисленных программ:

Науки о данных	https://priem.mipt.ru/applications_v2/bWFzdGVyL05hdWtpIG8gZGFubnlraF9Lb250cmFrdC5odG1s
Современная комбинаторика	https://priem.mipt.ru/applications_v2/bWFzdGVyL1NvdnJlbWVubmF5YSBrb21iaW5hdG9yaWthX0tvbnRyYWt0Lmh0bWw=
Комбинаторика и цифровая экономика	https://priem.mipt.ru/applications_v2/bWFzdGVyL0tvbWJpbmF0b3Jpa2EgaSB0c2lmcm92YXlhIGVrb25vbWlrYV9Lb250cmFrdC5odG1s
Contemproary combinatorics	https://priem.mipt.ru/applications_v2/bWFzdGVyL0NvbnRlbXBvcmFyeSBTb21iaW5hdG9yaWNzX0tvbnRyYWt0Lmh0bWw=
Modern Artificail Intelligence	https://priem.mipt.ru/applications_v2/bWFzdGVyL01vZGVybiBzdGF0ZSBvZiBBcnRpZmljaWFsIEludGVsbGlnZW5jZV9Lb250cmFrdC5odG1s
Разработка IT-продукта	https://priem.mipt.ru/applications_v2/bWFzdGVyL1JhenJhYm90a2EgSVQtcHJvZHVrdGFfS29udHJha3QuaHRtbA==
Управление IT-продуктами	https://priem.mipt.ru/applications_v2/bWFzdGVyL1VwcmF2bGVuaWUgSVQtcHJvZHVrdGFtaV9Lb250cmFrdC5odG1s

## МИФИ

— смотрим на последний элемент с классом «trPosBen», в нём нас интересует значение в div с классом «pos»
— например <td class="pos">236</td> — забираем 236
— такое нужно сделать для следующих программ:

Машинное обучение	https://org.mephi.ru/pupil-rating/get-rating/entity/12843/original/no
Науки о данных	https://org.mephi.ru/pupil-rating/get-rating/entity/12842/original/no
Кибербезопаность	https://org.mephi.ru/pupil-rating/get-rating/entity/12847/original/no
Безопасность информационных систем	https://org.mephi.ru/pupil-rating/get-rating/entity/12846/original/no
Разработка програмного обеспечения	https://org.mephi.ru/pupil-rating/get-rating/entity/12844/original/no
Разработка веб приложений	https://org.mephi.ru/pupil-rating/get-rating/entity/12845/original/no